前期准备
  * 修改netcorek8s项目的ValuesController文件，代码如下：
    ```
    public ActionResult<string> Get()
    {
        return Content("v1 from "+System.Environment.MachineName);
    }
    ```
  * 编译发布，构建镜像refactor2/netcorek8s:v1，上传至dockerhub
  * 将代码改成` return Content("v2 from "+System.Environment.MachineName);`，构建镜像refactor2/netcorek8s:v2，上传至dockerhub

手动更新应用的三种方式
  * 删除所有pod，由ReplicaSet使用新的镜像重建pod，缺点：会导致一段时间内不可用
      * `kubectl create -f netcorek8s-rs-v1.yml`，创建名为netcorek8srs的ReplicaSet，选择器是{key: apprs, operator: In, values: [netcorek8s]}，镜像refactor2/netcorek8s:v1
      * `kubectl create -f netcorek8s-svc-nodeport.yml`，创建名为netcorek8ssvc的Service，选择器是apprs: netcorek8s
      * `kubectl edit rs netcorek8srs`，更新image: refactor2/netcorek8s:v2，访问http://192.168.99.103:32000/api/values，发现都是v1版本
      * `kubectl delete pod -l apprs=netcorek8s`，删除标签apprs: netcorek8s的pod，由ReplicaSet重建pod，访问http://192.168.99.103:32000/api/values，发现都是v2版本
  * 先创建新的pod，修改service的选择器，删除所有的pod，蓝绿部署，缺点：会占用双倍资源
      * `kubectl delete -f netcorek8s-rs-v1.yml`，删除已修改的netcorek8srs，确保环境干净
      * `kubectl create -f netcorek8s-rs-v1.yml`，创建名为netcorek8srs的ReplicaSet
      * `kubectl create -f netcorek8s-rs-v2.yml`，创建名为netcorek8srs-v2的ReplicaSet，镜像，选择器都改成v2
      * `kubectl edit svc netcorek8ssvc-nodeport`，修改选择器apprs: netcorek8s-v2
      * `kubectl delete -f netcorek8s-rs-v1.yml`，删除名为netcorek8srs的ReplicaSet
  * 删除一个旧pod，启用一个新的pod，交替进行
      * `kubectl delete -f netcorek8s-rs-v2.yml`，`kubectl delete -f netcorek8s-svc-nodeport.yml`，还原环境
      * `kubectl create -f netcorek8s-rc-service-v1.yml`，创建名为netcorek8src-v1的ReplicationController和service
      * `kubectl rolling-update netcorek8src-v1 netcorek8src-v2 --image=refactor2/netcorek8s:v2  --v 6`，使用滚动更新，只支持ReplicationController，不支持ReplicaSet，可以看出这是在客户端请求一系列的API server，执行过程可能会受到网络的影响，导致执行不成功，或者整个流程执行一半

Deployment
Deployment是为了部署和升级应用，在ReplicaSet上级层次构建的kubernetes 资源。当创建一个Deployment时，ReplicaSet会被自动创建。当使用Deployment时，托管的pod实际是被ReplicaSet管理，而不是被Deployment管理。
  * 实例
      * `kubectl delete rc netcorek8src-v2`，删除存在的ReplicationController，确保环境干净
      * `kubectl create -f netcorek8s-deployment-v1.yml --record`，创建deployment
      * `kubectl get pod`，`kubectl get rs`，查看创建的pod和ReplicaSet
      * `kubectl rollout status deployment netcorek8s-deployment`，查看netcorek8s-deployment状态
  * 部署策略
      * RollingUpdate，滚动更新，默认
      * Recreate，删除pod重建，类似与手动更新应用的第一种方式
  * `kubectl patch deployment netcorek8s-deployment --patch "{\"spec\": {\"minReadySeconds\": 10}}"`，为deployment添加属性，放慢更新速度，[参考](https://docs.okd.io/latest/minishift/openshift/openshift-client-binary.html#example-config-cors)
  * `kubectl patch`命令在修改单个属性或者资源数量时非常有用，因为不必使用`kubectl edit`打开默认编辑器。如果使用 `kubectl patch`没有更新定义的pod  template，只是更新其他字段，如期望数量，部署策略等，都不会触发更新
  * `kubectl set image deployment netcorek8s-deployment netcorek8s=refactor2/netcorek8s:v2`，修改容器netcorek8s镜像为refactor2/netcorek8s:v2
  * 如果在deployment的pod template里引用了ConfigMap/Secret，修改ConfigMap不会触发更新，可以通过新创建一个ConfigMap，然后在pod template里引用新的ConfigMap来触发更新
  * 使用deployment滚动更新执行步骤类似于`kubectl rolling-update`，只不过是在服务端执行的。可以通过`kubectl get rs`，发现存在两个ReplicaSet，但是不会像`kubectl rolling-update`执行完后删除旧的ReplicationController，这是因为这些自动创建的ReplicaSet后面会用作回滚用，如图：  
    ![rs.png](https://images.gitee.com/uploads/images/2019/0226/210206_8e54beb7_5849.png "rs.png")
  * 使用deployment自动创建的ReplicaSet，每个deployment默认会保留2个ReplicaSet，可以通过revisionHistoryLimit属性设置
  * `kubectl rollout undo deployment netcorek8s-deployment`，回滚到上一版本
  * `kubectl rollout history deployment netcorek8s-deployment`，查看历史版本信息，如果在创建deployment时没有指定`--record`时，则看不到详细信息，如图：  
    ![rollout-history.png](https://images.gitee.com/uploads/images/2019/0226/210149_d5f6f86b_5849.png "rollout-history.png")
  * `kubectl rollout undo deployment netcorek8s-deployment --to-revision=1`，回滚到指定版本
  * 控制滚动更新的速率，可以通过maxSurge和maxUnavailable属性来设置，如
    ```
    spec:
      strategy:
          rollingUpdate:
            maxSurge: 1
            maxUnavailable: 0
          type: RollingUpdate
    ```  
      * maxSurge，设置可以有多少个pod超过期望的pod的数量，默认值25%，即最多有25%的pod数量超过期望的pod的数量。如果设置的期望数量是4，那在更新时最多有5个pod同时运行，如果得到是小数，则向下取整。可以将百分比改成固定值，即可以比期望pod数量多的值，如设置为1，则最多可以有多于期望数量1个pod
      * maxUnavailable，设置可以有多少个不可用的pod相对于期望的pod的数量，默认值25%。如果设置的期望数量是4，那在更新时最多有1个pod不可访问，如果得到是小数，则向下取整。可以将百分比改成固定值，如设置为1，则最多可以有1个pod不可访问
  * `kubectl rollout pause deployment netcorek8s-deployment`，暂停更新，先让部分pod更新
  * `kubectl rollout resume deployment netcorek8s-deployment`，恢复更新
  * 金丝雀发布，目前可以使用两个不同的Deployment，合适的操作scale来实现
  * `minReadySeconds`属性可以放慢更新速度，它最主要的作用是防止部署了一个故障的版本
      * `kubectl apply -f netcorek8s-readiness.yml`，更新netcorek8s-deployment，添加readiness，设置maxSurge: 1，maxUnavailable: 0，minReadySeconds: 10
      * `kubectl rollout status deployment netcorek8s-deployment`，查看更新状态，一直在等待，如图：  
       ![rollout-status.png](https://images.gitee.com/uploads/images/2019/0226/210158_9ca694e6_5849.png "rollout-status.png")
      * `kubectl get pod`，查看pod，显示未ready，如图：  
       ![pod.png](https://images.gitee.com/uploads/images/2019/0226/210141_3219dd4e_5849.png "pod.png")
      * 在新pod启动时，readiness探针开始执行，每秒执行一次，执行3次后，readiness探针返回失败，将pod从service的endpoint中移除，标记pod为未ready。rollout status显示只有一个pod被启动，应用更新被停止，因为新pod一直不可用。如果新pod要想变成可用，它需要变成ready至少10s(minReadySeconds)，一直到新pod变成可用，更新程序不会在创建新的pod，也不会移除老的pod，因为指定了只有0(maxUnavailable)个pod不可用
      * 如果只指定了readiness探针，没有适当的指定minReadySeconds，如果pod的readiness返回成功会被立即认为ready。readiness探针后面再返回失败，更新程序可能已经更新完了，所以要适当的指定minReadySeconds
      * `kubectl describe deploy netcorek8s-deployment`，发现更新程序超时ProgressDeadlineExceeded，如图：  
       ![describe.png](https://images.gitee.com/uploads/images/2019/0226/210133_def93f70_5849.png "describe.png")
      * 可以通过指定progressDeadlineSeconds属性来设置更新超时时间



修改资源的常用方式
  * `kubectl edit`，在默认编辑器里打开资源的定义，修改后，保存，退出编辑器，资源的定义就会被更新，如` kubectl edit deployment netcorek8s-deployment`
  * `kubectl patch`，修改资源的单个属性，如` kubectl patch deployment kubia -p '{"spec":{"template": {"spec": {"containers": [{"name":"netcorek8s", "image": "refactor2/netcorek8s:v2"}]}}}}'`
  * `kubectl apply`，从完整的yaml文件中，创建会修改资源，如`kubectl apply -f netcorek8s-deployment-v1.yml`
  * `kubectl replace`，从完整的yaml文件中，替换成新对象，要求原来的对象必须存在，`kubectl replace -f netcorek8s-deployment-v1.yml`
  * `kubectl set image`，修改pod里的image定义，如`kubectl set image deployment netcorek8s-deployment netcorek8s=refactor2/netcorek8s:v2`
